# Medallion Architecture With Apache Spark Code
**Project:** Earthquake Analytics in Microsoft Fabric (Bronze → Silver → Gold)  
**Workspace:** `Earthquake_API_Visualization`  
**Lakehouse:** `Earthquake_Lakehouse`  
**Pipeline:** `Earthquake_Pipeline`  
**Data Source:** USGS Earthquake API (GeoJSON)

---

1) Overview
This project demonstrates an end-to-end Microsoft Fabric implementation of **Medallion Architecture** using **Apache Spark (Fabric Notebooks)**:

- **Bronze:** Ingest raw API data and land it as raw JSON in Lakehouse Files
- **Silver:** Flatten and structure JSON into a typed Delta table
- **Gold:** Apply business-ready logic for reporting (classification + curated dataset)
- **Orchestration:** Fabric Data Factory pipeline runs Bronze → Silver → Gold on a schedule
- **Reporting:** Power BI report built from a semantic model over the Gold table  
  - Map visuals required tenant setting enablement
  - Reverse geocoding libraries were not available in this Fabric environment, so the report uses latitude/longitude and DAX for presentation fixes where needed

---

2) Fabric Setup Steps
1. Create a Fabric workspace named: `Earthquake_API_Visualization`
2. Create a Lakehouse named: `Earthquake_Lakehouse`
3. Create three notebooks:
   - `Bronze_Layer`
   - `Silver_Layer`
   - `Gold_Layer`
4. Attach each notebook to the same Lakehouse: `Earthquake_Lakehouse`

---

3) Medalian Architecture(Bronze-Silver-Gold)

Bronze Layer — API Ingestion (Raw JSON)
**Goal:** Pull earthquake data from USGS and save raw JSON to Lakehouse Files.

###Bronze Notebook Code (`Bronze_Layer`)

import requests
import json
from datetime import date, timedelta

#Data Factory Pipeline(Manual Run Fallback)
start_date = date.today() - timedelta(7) # 7 days
end_date = date.today() - timedelta(1)

# Construct the API URL with start and end dates provided by Data Factory, formatted for geojson output.
url = f"https://earthquake.usgs.gov/fdsnws/event/1/query?format=geojson&starttime={start_date}&endtime={end_date}"

try:
    # Make the GET request to fetch data
    response = requests.get(url)

    # Check if the request was successful
    response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)
    data = response.json().get('features', [])

    if not data:
        print("No data returned for the specified date range.")
    else:
        # Specify the file name (and path if needed)
        file_path = f'/lakehouse/default/Files/{start_date}_earthquake_data.json'

        # Save the JSON data
        with open(file_path, 'w') as file:
            json.dump(data, file, indent=4)
        print(f"Data successfully saved to {file_path}")
except requests.exceptions.RequestException as e:
    print(f"Error fetching data from API: {e}")
----------------------------------------------------------------------------------------------------------------
##Silver Notebook Code (`Silver_Layer`)
from pyspark.sql.functions import col, isnull, when
from pyspark.sql.types import TimestampType
from datetime import date, timedelta

#Data Factory Pipeline(Manual Run Fallback)
start_date = date. today() - timedelta(7)

# Load the JSON data into a Spark DataFrame
df = spark.read.option("multiline", "true").json(f"Files/{start_date}_earthquake_data.json")

# Reshape earthquake data
df = (
    df
    .select(
        'id',
        col('geometry.coordinates').getItem(0).alias('longitude'),
        col('geometry.coordinates').getItem(1).alias('latitude'),
        col('geometry.coordinates').getItem(2).alias('elevation'),
        col('properties.title').alias('title'),
        col('properties.place').alias('place_description'),
        col('properties.sig').alias('sig'),
        col('properties.mag').alias('mag'),
        col('properties.magType').alias('magType'),
        col('properties.time').alias('time'),
        col('properties.updated').alias('updated')
    )
)

# Validate data: Check for missing or null values
df = (
    df
    .withColumn('longitude', when(isnull(col('longitude')), 0).otherwise(col('longitude')))
    .withColumn('latitude', when(isnull(col('latitude')), 0).otherwise(col('latitude')))
    .withColumn('time', when(isnull(col('time')), 0).otherwise(col('time')))
)

# Convert 'time' and 'updated' to timestamp
df = (
    df
    .withColumn('time', (col('time') / 1000).cast(TimestampType()))
    .withColumn('updated', (col('updated') / 1000).cast(TimestampType()))
)

# Append to the silver table
df.write.mode('append').saveAsTable('earthquake_events_silver')


----------------------------------------------------------------------------------------------------------------------
##Gold Notebook Code (`Gold_Layer`)

from pyspark.sql.functions import when, col, udf
from pyspark.sql.types import StringType
# Ensure the below library is installed on your fabric environment
import reverse_geocoder as rg
from datetime import date, timedelta

#Data Factory Pipeline(Manual Run Fallback)
start_date = date. today() - timedelta(7)

df = spark.read.table("earthquake_events_silver").filter(col('time') > start_date)
     

def get_country_code(lat, lon):
    """
    Retrieve the country code for a given latitude and longitude.

    Parameters:
    lat (float or str): Latitude of the location.
    lon (float or str): Longitude of the location.

    Returns:
    str: Country code of the location, retrieved using the reverse geocoding API.

    Example:
    >>> get_country_details(48.8588443, 2.2943506)
    'FR'
    """
    coordinates = (float(lat), float(lon))
    return rg.search(coordinates)[0].get('cc')
     

# registering the udfs so they can be used on spark dataframes
get_country_code_udf = udf(get_country_code, StringType())
     

# adding country_code and city attributes
df_with_location = \
                df.\
                    withColumn("country_code", get_country_code_udf(col("latitude"), col("longitude")))
     

# adding significance classification
df_with_location_sig_class = \
                            df_with_location.\
                                withColumn('sig_class', 
                                            when(col("sig") < 100, "Low").\
                                            when((col("sig") >= 100) & (col("sig") < 500), "Moderate").\
                                            otherwise("High")
                                            )
     

# appending the data to the gold table
df_with_location_sig_class.write.mode('append').saveAsTable('earthquake_events_gold')

------------------------------------------------------------------------------------------------------------------------

4) Reverse Geocoding Workaround (Environment Constraint)

Attempted enhancement: enrich earthquakes with country/state using reverse_geocoder.

What happened:

Fabric Environment did not allow the external package install in this tenant/capacity

Result: ModuleNotFoundError: No module named 'reverse_geocoder'

Workaround:

Use latitude/longitude directly in Power BI Map visual

Use DAX to fill missing location labels when needed

This is a realistic platform constraint and is documented as a design decision.
---------------------------------------------------------------------------------------------------------------------------

5) Fabric Data Factory Pipeline — Orchestration

Goal: Automate Bronze → Silver → Gold via a parameterized pipeline.

Pipeline Steps

Create a new pipeline named: Earthquake_Pipeline

Add pipeline parameters:

p_start_date (String)

p_end_date (String)

Suggested defaults (testing):

p_start_date:

@formatDateTime(addDays(utcNow(), -7), 'yyyy-MM-dd')

p_end_date:

@formatDateTime(addDays(utcNow(), -1), 'yyyy-MM-dd')

Add 3 Notebook activities in order:

Activity 1: Bronze

Activity name: Bronze_Notebook

Notebook: Bronze_Layer

Parameters:

start_date = @pipeline().parameters.p_start_date

end_date = @pipeline().parameters.p_end_date

Activity 2: Silver (depends on Bronze success)

Activity name: Silver_Notebook

Notebook: Silver_Layer

Parameters:

start_date = @pipeline().parameters.p_start_date

Activity 3: Gold (depends on Silver success)

Activity name: Gold_Notebook

Notebook: Gold_Layer

Parameters:

start_date = @pipeline().parameters.p_start_date

end_date = @pipeline().parameters.p_end_date (optional unless used)

Save and run once manually to validate

Add a daily schedule trigger (optional)

Trigger does not add incremental cost beyond capacity usage

Use your local time zone

Common Issue Encountered:

Error: Parameter p_start_date was not found under Earthquake_Pipeline

Fix: ensure p_start_date exists at the pipeline level (not just in activity mappings) and Save/Publish.
-----------------------------------------------------------------------------------------------------------

6) Power BI Reporting Steps (Semantic Model → Report)

In Lakehouse, switch to SQL analytics endpoint

(Create a new semantic mode)l

select earthquake_events_gold

(Name semantic model: Earthquake_Report)

Switch to Power BI experience

(Create New report using the semantic model)

Map visual:

tenant setting needed: enable Map and Filled Map visuals

use Latitude and Longitude fields for accurate plotting

DAX used as needed to fill missing location labels (presentation-layer fix)
------------------------------------------------------------------------------------------------------------

7) Conclusion: What’s Delivered

Automated pipeline (Bronze → Silver → Gold)

Curated Gold table for reporting

Power BI report built on Gold semantic model

Documented decisions and real-world constraints (reverse geocoding + map settings)
